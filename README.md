# FBHash : A New Similarity Hashing Scheme for Digital Forensics 
## [D. Chang et al. / Digital Investigation 29 (2019) S113eS123]

This is an implementation of the mentioned paper. The core idea behind the FbHashing scheme is to identify frequently occuring
chunks in document and use it to find the similarity of the document with another document. In another terms, it is used to 
fingerprint a document to identify interesting pieces of information that will aid in digital forensic investigations. 

The whole process occurs as follows.
- Take two document of length N. For our ease of demonstration, I've added two simple data objects within the source code itself.
Each of them are 52 characters long and include only the uppercase and lowercase ASCII characters. 
- For each document do the following
- First find the chunks in a document. For that we slice the document into numerous pieces with length 7. Each new chunk is generated by removing the first character from the preceeding chunk and adding another one at the end from the document supplied. For example, if our document was ABCDEFGHIJKLM... then the first chunk will be ABCDEFG and the second one will be BCDEFGH and so on. 
- The chunks are then passed on to the Rabin-Karp Rolling Hash function. Here a unique hash of the chunks are calculated using the following equation : 

#### H = C<sub>1</sub>a<sup>(k-1)</sup> + C<sub>2</sub>a<sup>(k-2)</sup> + C<sub>3</sub>a<sup>(k-3)</sup> + ... + C<sub>k</sub>a<sup>0</sup> modulus n
Where "a" is a constant, k is window size (chunk length), n is a large prime number, 

C<sub>1</sub>,C<sub>2</sub>,..,C<sub>k</sub> are the input characters (ASCII Values)

Hnew = ( aH - C<sub>0</sub>a<sup>k</sup> + incoming byte ) modulus n

EQN : RollingHash(Ch<sub>(i+1)</sub>) = a*RollingHash(Ch<sub>i</sub>) - C<sub>i</sub>a<sup>k</sup> + C<sub>(i+k)</sub> modulus n

Max value of H can be an unsigned 64 bit number

All possible ASCII values of C<sub>i</sub> (input characters) = 256 (ASCII Value)

All possible values of "a" (constant) = 256 (size of alphabet)

Value of k should satisfy the following

   2<sup>64</sup> - 1 >= 256 * 256<sup>(k-1)</sup>

Let k = 7

   2<sup>64</sup> > 2<sup>8</sup> * (2<sup>8</sup>)<sup>6</sup> = 2<sup>56</sup>
And n = 801385653117583579  :  Prime number larger than 2^56 = 72057594037927940. This huge value is necessary to ensure that there is no collision while generating hashes. 

- Using this list of rolling hashes, filter out the unique chunks and find the chunk frequncy. By chunk frequency we mean the number of times a chunk repeats itself within the same document. This functions a hash table with its indexes being the rolling hashes and the values associated being the frequency of the chunk.
- Once the chunk frequency is calculated, the chunk weight is calculated using it. A weight is assigned to the each chunk based on its frequency. The weight is calculated using the following equation.

#### Chunk Weight of i<sup>th</sup> chunk = 1 + log<sub>10</sub>(Ch<sub>i</sub>f<sup>D</sup>)

Where Ch<sub>i</sub>f<sup>D</sup> is the chunk frequency of that i<sup>th</sup> chunk

- The value of a chunk is determined by the frequency of the chunk in other documents as well. More frequent the chunk is, less valuable it become. To determine the value of a chunk, the rolling hashes of a chunk is checked with the rolling hashes of other documents in the database and its doucument frequency is calculated just like the chunk frequency is calculated.
Docuement frequency is represented as d<sup>f</sup>Ch
- Using the document frequency, the document weight is calculated. The document weight is know as the informativeness fo the document. As mentioned above, if the document frequency of a chunk is high, then it is less valuable. The less frequent chunks are more interesting for fingerprinting documents. The document weight can be canculated as follows.

#### id<sup>f</sup>Ch<sub>i</sub> = log<sub>10</sub>(N/d<sup>f</sup>Ch<sub>i</sub>)
Where N is the number of documents the chunk was checked against for calculating the document frequency. For ease of demonstration, we are generating the N documents withing the code itself using the randStr() function on the top of the source code. In real life these documents would be fetched from a database. 

NB : In our case we are generating a 1000 data objects using this function and using it to calculate the document frequency. This have a few problems. The results may vary with each run for the same input document. A weird looking edge case will occur as well because 3-4 extra documents were manually added to the set for testing purposes. This will cause a minor difference in the last similary percentage calculated. In one of the test runs which I made, both D1 and D2 were the same. Due to the above mentioned extra documents the similary score was 100.000000003%. This can be fixed by removing the extra datasets. I've left it there for ease of testing.

- Using the values we have obtained so far we can calculate the Chunk Score. The chunk score is the final relavence of a chunk. Its calculated as follows : 
#### Chunk Score W<sup>D</sup><sub>x</sub>Ch = Chwgt * id<sup>f</sup>Ch
Where x is the document number (that is 1 and 2 in D1 and D2 respectively)

- All the chunk scores are callectively used to represent the Digest of the Docuemt. The digest of the document is the chunk scores in vector form.

#### digest(D1) = W<sup>D</sup><sub>1</sub>C<sub>0</sub>, W<sup>D</sup><sub>1</sub>C<sup>2</sup>, ..., W<sup>D</sup><sub>1</sub>C<sub>h-1</sub>


- All these calculations were carried out to find the similarity between the documents D1 and D2. The equation mentioned below returns the similarity in the scale from 0 to 100 percentage. 

<a href="https://www.codecogs.com/eqnedit.php?latex=Similarity(D1,&space;D2)&space;=&space;\frac{\sum_{i&space;=&space;0}^{n&space;-&space;1&space;}{W^{D}}_{1}C_{i}*{W^{D}}_{2}C_{i}}{\sqrt{({W^{D}}_{1}C_{i})^2}*\sqrt{({W^{D}}_{1}C_{i})^2}}*100" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Similarity(D1,&space;D2)&space;=&space;\frac{\sum_{i&space;=&space;0}^{n&space;-&space;1&space;}{W^{D}}_{1}C_{i}*{W^{D}}_{2}C_{i}}{\sqrt{({W^{D}}_{1}C_{i})^2}*\sqrt{({W^{D}}_{1}C_{i})^2}}*100" title="Similarity(D1, D2) = \frac{\sum_{i = 0}^{n - 1 }{W^{D}}_{1}C_{i}*{W^{D}}_{2}C_{i}}{\sqrt{({W^{D}}_{1}C_{i})^2}*\sqrt{({W^{D}}_{1}C_{i})^2}}*100" /></a>
